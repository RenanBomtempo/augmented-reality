{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabalho Prático 2: Realidade Aumentada\n",
    "\n",
    "## Alunos: \n",
    "\n",
    "- Felipe Eduardo dos Santos - 2017021223\n",
    "- Renan Antunes Braga Bomtempo - 2018048524"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependências e funções auxiliares\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "# OpenCV\n",
    "import cv2\n",
    "\n",
    "# OpenGL\n",
    "from OpenGL.GL import *\n",
    "from OpenGL.GLUT import *\n",
    "from OpenGL.GLU import *\n",
    "\n",
    "import numpy as np\n",
    "from objloader import *\n",
    "\n",
    "# Calcula erro medio quadratico\n",
    "def calc_rmse(predictions, targets):\n",
    "    return np.sqrt(((predictions - targets) **2).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibração da câmera\n",
    "\n",
    "Para calibrar a câmera, extraimos 24 frames do video (1 frame de cada segundo do vídeo), e utilizamos as imagens no Toolkit disponibilizado para Octave. Com os parâmetros intrínsecos da câmera em mãos, podemos montar nossa matriz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focal length\n",
    "foc_len = (1203.33680, 1205.25410)\n",
    "\n",
    "# Principal point\n",
    "pri_pt  = (959.50000,  539.50000)\n",
    "\n",
    "# Distortion coefficients\n",
    "dist_coef = np.array([0.07433, -0.17385, -0.00486, 0.00222, 0.],dtype=\"float32\")\n",
    "\n",
    "# Camera matrix\n",
    "camera_matrix = np.array([[foc_len[0],          0., pri_pt[0]],\n",
    "                          [        0.,  foc_len[1], pri_pt[1]], \n",
    "                          [        0.,          0.,        1.]],dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurações da GLUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewport dimensions\n",
    "dimensions = (1920, 1080)\n",
    "\n",
    "# GLUT configuration\n",
    "glutInit()\n",
    "glutInitDisplayMode(GLUT_RGBA | GLUT_DOUBLE)\n",
    "glutSetOption(GLUT_ACTION_ON_WINDOW_CLOSE, GLUT_ACTION_CONTINUE_EXECUTION)\n",
    "glutInitWindowSize(*dimensions)\n",
    "window = glutCreateWindow(b'Realidade Aumentada')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definindo as funções\n",
    "## Inicialização dos parâmetros da OpenGL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(width, height) = dimensions\n",
    "\n",
    "glClearColor(0.0, 0.0, 0.0, 0.0)\n",
    "glClearDepth(1.0)\n",
    "glEnable(GL_DEPTH_TEST)\n",
    "\n",
    "glMatrixMode(GL_PROJECTION)\n",
    "glLoadIdentity()\n",
    "\n",
    "fx = foc_len[0]\n",
    "fy = foc_len[1]\n",
    "\n",
    "fovy = 2*np.arctan(0.5*height/fy)*180/np.pi\n",
    "aspect = (width*fy)/(height*fx)\n",
    "gluPerspective(fovy, aspect, 0.01, 200.0)\n",
    "\n",
    "glMatrixMode(GL_MODELVIEW)\n",
    "\n",
    "def idleCallback():\n",
    "    glutPostRedisplay()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encontrando o alvo na cena\n",
    "Dado um frame do video, queremos encontrar onde estão os alvos (coordenadas das 4 quinas do alvo). Para isso nós:\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux = 0\n",
    "\n",
    "# Verifica se um candidato a alvo é um alvo, e se sim, qual sua orientecao\n",
    "def trackerOrient(img, alvo, points,frame):\n",
    "    global aux\n",
    "    threshold = 127\n",
    "\n",
    "    palvo = np.ones(alvo.shape)\n",
    "    palvoPoints = np.array([[0,0],[0,alvo.shape[1]],\n",
    "                            [alvo.shape[0],alvo.shape[1]],[alvo.shape[0],0]], dtype=\"float64\")\n",
    "\n",
    "    h, status = cv2.findHomography(points, palvoPoints)\n",
    "    palvo = cv2.warpPerspective(img, h, (palvo.shape[0], palvo.shape[1]))\n",
    "\n",
    "    alvo0 = cv2.cvtColor(alvo, cv2.COLOR_BGR2GRAY)\n",
    "    _, alvo0 = cv2.threshold(alvo0, threshold, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Criando os alvos rotacionados\n",
    "    alvo1 = cv2.rotate(alvo0, cv2.ROTATE_90_CLOCKWISE)\n",
    "    alvo2 = cv2.rotate(alvo1, cv2.ROTATE_90_CLOCKWISE)\n",
    "    alvo3 = cv2.rotate(alvo2, cv2.ROTATE_90_CLOCKWISE)\n",
    "\n",
    "\n",
    "    alvos = [alvo0,alvo1,alvo2,alvo3]\n",
    "\n",
    "    # Verificando qual orientação produz o menor erro quadratico\n",
    "    orient = -1\n",
    "    minErr = 999\n",
    "    for i in range(4):\n",
    "        err = calc_rmse(palvo,alvos[i])\n",
    "        if err<4 and err<minErr:\n",
    "            minErr = err\n",
    "            orient = i\n",
    "\n",
    "    # No vetor de pontos os poontos estão em sentido anti-horario, mas enumerei\n",
    "    # As possiveis rotções no sentido horario, então esse order inverte isso\n",
    "    orderAnti = [0,3,2,1]\n",
    "    orient = orderAnti[orient]\n",
    "\n",
    "    # Origem\n",
    "    if orient>=0:\n",
    "        #origem = order[orient]\n",
    "        return True, orient\n",
    "\n",
    "    return False,(0,0)\n",
    "    #--------------------------------------------------------------------------\n",
    "    # Marcar na imagem qual a origem estimada\n",
    "    # order = [points[0][0],points[3][0],points[2][0],points[1][0]]\n",
    "    # frame = cv2.circle(frame, tuple(order[orient]), 10, (255,0,0), -1)\n",
    "\n",
    "\n",
    "    aux += 1\n",
    "\n",
    "def detectMarkers(img):\n",
    "    # assumindo que o alvo é quadrado\n",
    "    alvo = cv2.imread('alvo.jpg')\n",
    "    alvo = cv2.resize(alvo,(322,322),interpolation = cv2.INTER_AREA)\n",
    "\n",
    "    # Binarização\n",
    "    threshold = 127\n",
    "\n",
    "    # Área mínima para ser detectada\n",
    "    ax = 100*100\n",
    "\n",
    "    imgGray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    _,imgBin = cv2.threshold(imgGray, threshold, 255, cv2.THRESH_BINARY)\n",
    "    imgCanny = cv2.Canny(imgBin, 100, 200)\n",
    "\n",
    "    # Dilatar as bordas\n",
    "    kernel = np.ones((5, 5))\n",
    "    imgCanny = cv2.dilate(imgCanny, kernel, iterations=1)\n",
    "\n",
    "    # Encontra os contornos da imagem. RETR_EXTERNAL apenas os mais externos RETR_TREE todos\n",
    "    contours, _ = cv2.findContours(imgCanny, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "    markers = []\n",
    "\n",
    "    numTrakers = 0\n",
    "    for contour in contours:\n",
    "        approx = cv2.approxPolyDP(contour, 0.01 * cv2.arcLength(contour, True), True)\n",
    "\n",
    "        # Caso a curva tiver 4 lados, for convexa e possui uma area minima de ax pixels\n",
    "        if len(approx) == 4 and cv2.isContourConvex(approx) and cv2.contourArea(approx)>ax:\n",
    "            # Ordem em que os vertices foram escritos\n",
    "            # drawOrder(img, approx)\n",
    "            istraker, indice_origem = trackerOrient(imgBin, alvo, approx,img)\n",
    "\n",
    "            if istraker:\n",
    "                numTrakers += 1\n",
    "                origem = approx[indice_origem][0]\n",
    "                eixox = [origem, approx[(indice_origem+1)%4][0]]\n",
    "                eixoy = [origem, approx[(indice_origem-1)%4][0]]\n",
    "\n",
    "                approx_fixed = []\n",
    "                for i in range(4):\n",
    "                    approx_fixed.append([approx[(indice_origem+i)%4][0][0],approx[(indice_origem+i)%4][0][1]])\n",
    "\n",
    "\n",
    "                # markers.append((approx, indice_origem))\n",
    "                markers.append((approx_fixed, indice_origem))\n",
    "                \n",
    "                # Desenhar na exixos na tela\n",
    "                cv2.drawContours(img, [approx], 0, (0, 255, 0), 1)\n",
    "                img = cv2.circle(img, tuple(origem), 10, (255, 0, 0), -1)\n",
    "\n",
    "                # Desenhar os eixos\n",
    "                img = cv2.line(img, tuple(eixox[0]), tuple(eixox[1]), (255,0,0), 6)\n",
    "                img = cv2.line(img, tuple(eixoy[0]), tuple(eixoy[1]), (0, 0, 255), 6)\n",
    "                # Colocar nome nos alvos\n",
    "                org = (approx[0][0][0]-50,approx[0][0][1]-50)\n",
    "                img = cv2.putText(img, 'Alvo '+str(numTrakers-1), org, cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                                    1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "                \n",
    "        # Desenhar todos os contornos\n",
    "        #cv2.drawContours(img, [approx], 0, (0, 0, 255), 1)\n",
    "    # print(\"Detected :\",numTrakers)\n",
    "    return markers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posicionar o Pikachu em cima do alvo\n",
    "\n",
    "Com o alvo detectado, e os parâmetros intrínsecos da câmera em mão, podemos encontrar a matriz de projeção que nos permitirá renderizar o modelo 3D em cima do alvo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def object3D(img_pts, orient, camera_matrix, dist_coef, obj):\n",
    "    # Marker points in world space\n",
    "    world_pts = np.array([[-1, -1, 0], [1, -1, 0],\n",
    "                          [ 1,  1, 0], [-1, 1, 0]], dtype=\"float32\")\n",
    "\n",
    "    # Marker points in image space\n",
    "    img_pts = np.array(img_pts, dtype=\"float32\")\n",
    "\n",
    "    # Calculate rotation and translation vectors\n",
    "    _, rot_vecs, t_vecs = cv2.solvePnP(world_pts, img_pts, camera_matrix, dist_coef)\n",
    "\n",
    "    # Generate rotation matrix\n",
    "    rot_m = cv2.Rodrigues(rot_vecs)[0]\n",
    "\n",
    "    # Build projection matrix\n",
    "    proj_matrix = np.array([[rot_m[0][0], rot_m[0][1], rot_m[0][2], t_vecs[0]],\n",
    "                            [rot_m[1][0], rot_m[1][1], rot_m[1][2], t_vecs[1]],\n",
    "                            [rot_m[2][0], rot_m[2][1], rot_m[2][2], t_vecs[2]],\n",
    "                            [        0.0,         0.0,         0.0,      1.0]], dtype=\"float32\")\n",
    "\n",
    "    # Matrix to convert between OpenCV and OpenGL coordinate systems\n",
    "    flip_yz = np.array([[1,  0,  0, 0],\n",
    "                        [0, -1,  0, 0],\n",
    "                        [0,  0, -1, 0],\n",
    "                        [0,  0,  0, 1]])\n",
    "\n",
    "    proj_matrix = np.dot(flip_yz, proj_matrix)\n",
    "    glLoadMatrixd(np.transpose(proj_matrix))\n",
    "\n",
    "    # Render model\n",
    "    glCallList(obj.gl_list)\n",
    "\n",
    "    # Render wire cube\n",
    "    #glutWireCube(2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Renderizar o frame do vídeo como background\n",
    "\n",
    "Função utilizada para projetar o frame do video como textura em um plano no OpenGL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawBackground(frame):\n",
    "    # Convert frame to OpenGL image \"format\"\n",
    "    background = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    background = cv2.flip(background, 0)\n",
    "\n",
    "    height, width, channels = background.shape\n",
    "    background = np.frombuffer(background.tobytes(), \n",
    "                               dtype=background.dtype, \n",
    "                               count=height * width * channels)\n",
    "    background.shape = (height, width, channels)\n",
    "\n",
    "    glBindTexture(GL_TEXTURE_2D, background_id)\n",
    "\n",
    "    glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_LINEAR)\n",
    "    glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_LINEAR)\n",
    "    glTexImage2D(GL_TEXTURE_2D, 0, 3, width, height, 0, GL_RGB, GL_UNSIGNED_BYTE, background)\n",
    "    glDepthMask(GL_FALSE)\n",
    "\n",
    "    glMatrixMode(GL_PROJECTION)\n",
    "    glPushMatrix()\n",
    "    glLoadIdentity()\n",
    "    gluOrtho2D(0, width, 0, height)\n",
    "\n",
    "    glMatrixMode(GL_MODELVIEW)\n",
    "    glBindTexture(GL_TEXTURE_2D, background_id)\n",
    "    glTexImage2D(GL_TEXTURE_2D, 0, 3, width, height, 0, GL_RGB, GL_UNSIGNED_BYTE, background)\n",
    "    glPushMatrix()\n",
    "\n",
    "    # Plane with video frame texture\n",
    "    glBegin(GL_QUADS)\n",
    "    glTexCoord2i(0, 0); glVertex2i(0, 0)\n",
    "    glTexCoord2i(1, 0); glVertex2i(width, 0)\n",
    "    glTexCoord2i(1, 1); glVertex2i(width, height)\n",
    "    glTexCoord2i(0, 1); glVertex2i(0, height)\n",
    "    glEnd()\n",
    "    glPopMatrix()\n",
    "\n",
    "    glMatrixMode(GL_PROJECTION)\n",
    "    glPopMatrix()\n",
    "\n",
    "    glMatrixMode(GL_MODELVIEW)\n",
    "    glDepthMask(GL_TRUE)\n",
    "    glDisable(GL_TEXTURE_2D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop de renderização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def displayCallback():\n",
    "    glMatrixMode(GL_MODELVIEW)\n",
    "    glLoadIdentity()\n",
    "    glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT)\n",
    "\n",
    "    # Habilita o uso de texturas\n",
    "    glEnable(GL_TEXTURE_2D)\n",
    "\n",
    "    read, frame = vid.read()\n",
    "\n",
    "    if read:\n",
    "        drawBackground(frame)\n",
    "\n",
    "        # Detect image markers\n",
    "        markers = detectMarkers(frame)\n",
    "        \n",
    "        # For each \n",
    "        for marker in markers:\n",
    "            # Place OBJ model on top of the marker\n",
    "            object3D(marker[0], marker[1], camera_matrix, dist_coef, obj)\n",
    "        glutSwapBuffers()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executando o aplicativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open input video\n",
    "vid = cv2.VideoCapture('tp2-icv-input.mp4')\n",
    "\n",
    "# Load Pikachu model\n",
    "obj = OBJ(\"Pikachu.obj\", swapyz=True)\n",
    "\n",
    "# Background \n",
    "background_id = glGenTextures(1)\n",
    "\n",
    "glutDisplayFunc(displayCallback)\n",
    "glutIdleFunc(idleCallback)\n",
    "\n",
    "glutMainLoop()\n",
    "\n",
    "vid.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
